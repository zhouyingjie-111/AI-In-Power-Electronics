"""
PPOå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–Buckå˜æ¢å™¨å‚æ•°è®¾è®¡
=====================================

æœ¬æ¨¡å—ä½¿ç”¨PPOç®—æ³•ä¼˜åŒ–Buckå˜æ¢å™¨çš„å…³é”®å‚æ•°,é€šè¿‡å¼ºåŒ–å­¦ä¹ å¯»æ‰¾æœ€ä¼˜çš„è®¾è®¡å‚æ•°ç»„åˆã€‚
ä¸»è¦åŠŸèƒ½ï¼š
1. å®šä¹‰Buckå˜æ¢å™¨è®¾è®¡å‚æ•°çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
2. ä½¿ç”¨é¢„è®­ç»ƒçš„ä»£ç†æ¨¡å‹é¢„æµ‹æ€§èƒ½æŒ‡æ ‡
3. é€šè¿‡PPOç®—æ³•ä¼˜åŒ–å‚æ•°,æœ€å¤§åŒ–æ•ˆç‡åŒæ—¶æ»¡è¶³çº¹æ³¢çº¦æŸ
4. æä¾›è®­ç»ƒè¿‡ç¨‹ç›‘æ§å’Œç»“æœå¯è§†åŒ–

"""

import numpy as np
import pandas as pd
import os
import sys
import random
import math
from typing import Tuple, Dict, List, Optional

# Force UTF-8 stdout/stderr to avoid UnicodeEncodeError on Windows GBK consoles
try:
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")  # type: ignore[attr-defined]
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")  # type: ignore[attr-defined]
except Exception:
    try:
        os.environ.setdefault("PYTHONIOENCODING", "utf-8")
        os.environ.setdefault("PYTHONUTF8", "1")
    except Exception:
        pass
os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '3')	# ä»…æ˜¾ç¤ºERRORï¼Œä¸”ä¸è¦†ç›–å¤–éƒ¨å·²è®¾ç½®
from keras.models import load_model
from sklearn.preprocessing import StandardScaler
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecNormalize
from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.font_manager as fm

"""è®¾ç½®å­—ä½“æ”¯æŒ"""
def setup_chinese_font():

    try:
        # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
        font_candidates = [
            'SimHei',           # Windows é»‘ä½“
            'Microsoft YaHei',  # Windows å¾®è½¯é›…é»‘
            'WenQuanYi Micro Hei',  # Linux æ–‡æ³‰é©¿å¾®ç±³é»‘
            'PingFang SC',      # macOS è‹¹æ–¹
            'Hiragino Sans GB', # macOS å†¬é’é»‘ä½“
            'Arial Unicode MS', # é€šç”¨
            'DejaVu Sans'       # å¤‡ç”¨
        ]
        
        # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
        chinese_font = None
        for font in font_candidates:
            if font in available_fonts:
                chinese_font = font
                break
        
        if chinese_font:
            matplotlib.rcParams['font.sans-serif'] = [chinese_font] + font_candidates
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
            matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
            
        matplotlib.rcParams['axes.unicode_minus'] = False
        
    except Exception as e:
        print(f"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤è®¾ç½®
        matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_font()

# ç¦ç”¨matplotlibè­¦å‘Š
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

"""
PPOè®­ç»ƒé…ç½®,é›†ä¸­ç®¡ç†æ‰€æœ‰å…³é”®å‚æ•°
"""
# é…ç½®å‚æ•°ä¸æ¨¡å‹åŠ è½½
class Config:

    # æ–‡ä»¶è·¯å¾„é…ç½®
    MODEL_PATH = 'E:/AI-based optimized design/Trained_model/trainedNet.keras'
    X_SCALER_PATH = 'E:/AI-based optimized design/Data/Input_Data/x_scaled_data.csv'
    Y_SCALER_PATH = 'E:/AI-based optimized design/Data/Input_Data/y_scaled_data.csv'
    HISTORY_PATH = 'E:/AI-based optimized design/Data/Training_History/training_history.npz'
    TENSORBOARD_LOG = "E:/AI-based optimized design/TensorBoard/PPO_Buck/"
    MODEL_SAVE_PATH = 'E:/AI-based optimized design/Trained_model/'
    CHECKPOINT_PATH = 'E:/AI-based optimized design/Trained_model/checkpoints/'
    VECNORM_PATH = 'E:/AI-based optimized design/Trained_model/vecnormalize.pkl'
    
    # äº¤é”™å¹¶è”Buckå˜æ¢å™¨å‚æ•°è®¾è®¡èŒƒå›´
    FIXED_FREQUENCY = 500e3         # å›ºå®šå¼€å…³é¢‘ç‡ (Hz)
    PARAM_BOUNDS = {
        'L(H)': (1e-6, 3e-6),       # ç”µæ„Ÿ (H)
        'C(F)': (8e-6, 10e-6),      # ç”µå®¹ (F)
        'Ron': (0.002, 0.005),         # å¼€å…³ç®¡å¯¼é€šç”µé˜» (Î©)
        'RL': (0.0015, 0.1),          # ç”µæ„Ÿç­‰æ•ˆä¸²è”ç”µé˜» (Î©)
        'RC': (0.01, 0.2)           # ç”µå®¹ç­‰æ•ˆä¸²è”ç”µé˜» (Î©)
    }
    
    # æ€§èƒ½çº¦æŸ
    RIPPLE_THRESHOLD = 0.005         # çº¹æ³¢ç³»æ•°ä¸Šé™ï¼ˆ0.5%ï¼‰
    MIN_EFFICIENCY = 0.75           # æœ€ä½æ•ˆç‡è¦æ±‚
    MAX_EFFICIENCY = 0.98           # æœ€é«˜æ•ˆç‡é™åˆ¶
    
    # PPOç®—æ³•å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    PPO_CONFIG = {
        'learning_rate': 5e-4,      # æé«˜å­¦ä¹ ç‡ï¼ŒåŠ å¿«æ”¶æ•›
        'n_steps': 2048,            # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
        'batch_size': 256,          # é€‚ä¸­æ‰¹æ¬¡ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
        'n_epochs': 15,             # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œæé«˜æ ·æœ¬åˆ©ç”¨ç‡
        'gamma': 0.99,              # æŠ˜æ‰£å› å­
        'gae_lambda': 0.95,         # GAEå‚æ•°
        'clip_range': 0.2,          # è£å‰ªèŒƒå›´
        'ent_coef': 0.005,          # é™ä½ç†µç³»æ•°ï¼ŒåŠ å¿«æ”¶æ•›
        'vf_coef': 0.5,             # ä»·å€¼å‡½æ•°ç³»æ•°
        'max_grad_norm': 0.5        # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    }
    
    # è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    MAX_STEPS_PER_EPISODE = 50      # æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
    EXPLORATION_RATE = 0.0          # ç¦ç”¨ç¯å¢ƒçº§éšæœºæ¢ç´¢ï¼ˆç”±ç­–ç•¥è´Ÿè´£ï¼‰
    SAVE_FREQUENCY = 100            # å†å²ä¿å­˜é¢‘ç‡
    EVAL_FREQUENCY = 1024           # æé«˜è¯„ä¼°é¢‘ç‡ï¼Œæ›´å¿«å‘ç°å¥½æ¨¡å‹
    CHECKPOINT_FREQUENCY = 2048     # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡

"""
åŠ è½½é¢„è®­ç»ƒçš„ä»£ç†æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
    
è¿”å›: Tuple[ä»£ç†æ¨¡å‹, è¾“å…¥æ ‡å‡†åŒ–å™¨, è¾“å‡ºæ ‡å‡†åŒ–å™¨]
"""
def load_surrogate_model() -> Tuple[object, StandardScaler, StandardScaler]:

    print("æ­£åœ¨åŠ è½½ä»£ç†æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨...")
    
    # åŠ è½½é¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹
    surrogate_model = load_model(Config.MODEL_PATH)
    print(f"âœ“ ä»£ç†æ¨¡å‹å·²åŠ è½½: {Config.MODEL_PATH}")
    
    # åŠ è½½è¾“å…¥æ ‡å‡†åŒ–å‚æ•°
    x_scaler_params = pd.read_csv(Config.X_SCALER_PATH)
    scaler_x = StandardScaler()
    scaler_x.mean_ = x_scaler_params['x_mu'].values
    scaler_x.scale_ = x_scaler_params['x_sigma'].values
    scaler_x.var_ = scaler_x.scale_ ** 2
    print(f"âœ“ è¾“å…¥æ ‡å‡†åŒ–å™¨å·²åŠ è½½: {Config.X_SCALER_PATH}")
    
    # åŠ è½½è¾“å‡ºæ ‡å‡†åŒ–å‚æ•°
    y_scaler_params = pd.read_csv(Config.Y_SCALER_PATH)
    scaler_y = StandardScaler()
    scaler_y.mean_ = y_scaler_params['y_mu'].values
    scaler_y.scale_ = y_scaler_params['y_sigma'].values
    scaler_y.var_ = scaler_y.scale_ ** 2
    print(f"âœ“ è¾“å‡ºæ ‡å‡†åŒ–å™¨å·²åŠ è½½: {Config.Y_SCALER_PATH}")
    
    return surrogate_model, scaler_x, scaler_y

# åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
surrogate_model, scaler_x, scaler_y = load_surrogate_model()

"""
Buckå˜æ¢å™¨è®¾è®¡ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
	
è¯¥ç¯å¢ƒå°†Buckå˜æ¢å™¨çš„è®¾è®¡å‚æ•°ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜:
- çŠ¶æ€ç©ºé—´:6ä¸ªè®¾è®¡å‚æ•° + 2ä¸ªæ€§èƒ½æŒ‡æ ‡(çº¹æ³¢ã€æ•ˆç‡)
- åŠ¨ä½œç©ºé—´:6ç»´è¿ç»­åŠ¨ä½œ,èŒƒå›´[-1, 1]ï¼Œæ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´
- å¥–åŠ±å‡½æ•°ï¼šåŸºäºæ•ˆç‡æœ€å¤§åŒ–ã€çº¹æ³¢çº¦æŸæ»¡è¶³ã€å‚æ•°å¤šæ ·æ€§
    
å…³é”®ç‰¹æ€§ï¼š
1. ä½¿ç”¨é¢„è®­ç»ƒä»£ç†æ¨¡å‹å¿«é€Ÿé¢„æµ‹æ€§èƒ½æŒ‡æ ‡
2. å¤šç›®æ ‡å¥–åŠ±å‡½æ•°å¹³è¡¡æ•ˆç‡ä¸çº¦æŸ
3. å†å²è®°å½•æ”¯æŒè®­ç»ƒè¿‡ç¨‹åˆ†æ
4. ç‰©ç†çº¦æŸç¡®ä¿è®¾è®¡å‚æ•°åˆç†æ€§
"""
# Buckå˜æ¢å™¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒæ­å»º
class BuckConverterEnv(gym.Env):

	def __init__(self, track_history: bool = True):
		super(BuckConverterEnv, self).__init__()
		self.track_history = track_history
		
		# å›ºå®šé¢‘ç‡
		self.fixed_frequency = Config.FIXED_FREQUENCY
		
		# åŠ¨ä½œç©ºé—´ï¼š5ç»´è¿ç»­åŠ¨ä½œï¼ˆç§»é™¤é¢‘ç‡ï¼‰ï¼ŒèŒƒå›´[-1, 1]
		self.action_space = spaces.Box(
			low=np.float32(np.array([-1.0] * 5)),
			high=np.float32(np.array([1.0] * 5)),
			dtype=np.float32
		)
		
		# è§‚æµ‹ç©ºé—´ï¼š7ç»´è¿ç»­çŠ¶æ€ 
		# å‰5ç»´ï¼šè®¾è®¡å‚æ•° [L, C, Ron, RL, RC];å2ç»´ï¼šæ€§èƒ½æŒ‡æ ‡ [ripple, efficiency]
		self.observation_space = spaces.Box(
			low=np.float32(-np.inf),
			high=np.float32(np.inf),
			shape=(7,),
			dtype=np.float32
		)

		# å‚æ•°è¾¹ç•Œå’Œåç§°
		self.param_bounds = np.array(list(Config.PARAM_BOUNDS.values()))
		self.param_names = list(Config.PARAM_BOUNDS.keys())
		self.param_ranges = self.param_bounds[:, 1] - self.param_bounds[:, 0]
		
		# æ€§èƒ½çº¦æŸ
		self.ripple_threshold = Config.RIPPLE_THRESHOLD
		self.min_efficiency = Config.MIN_EFFICIENCY
		self.max_efficiency = Config.MAX_EFFICIENCY
		
		# Episodeæ§åˆ¶
		self.max_steps = Config.MAX_STEPS_PER_EPISODE
		self.current_step = 0

		# å†å²è®°å½•åˆå§‹åŒ–ï¼ˆå§‹ç»ˆå…·å¤‡å±æ€§ï¼Œæ˜¯å¦è¯»å†™ç”± track_history æ§åˆ¶ï¼‰
		self.history_file = Config.HISTORY_PATH
		# å…ˆåˆå§‹åŒ–ä¸ºç©ºï¼Œç¡®ä¿å±æ€§å­˜åœ¨
		self.param_history = []
		self.ripple_history = []
		self.efficiency_history = []
		self.reward_history = []
		self.diversity_history = []
		self.boundary_distance_history = []
		self.step_count = 0
		# ä»…åœ¨è®­ç»ƒç¯å¢ƒæ—¶åŠ è½½å†å²
		if self.track_history:
			os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
			self.load_history()

	# åŠ è½½è®­ç»ƒå†å²
	def load_history(self):
		try:
			if os.path.exists(self.history_file):
				data = np.load(self.history_file)
				self.param_history = list(data['param_history'])
				self.ripple_history = list(data['ripple_history'])
				self.efficiency_history = list(data['efficiency_history'])
				self.reward_history = list(data['reward_history'])
				self.diversity_history = list(data['diversity_history'])
				self.boundary_distance_history = list(data['boundary_distance_history'])
				self.step_count = len(self.reward_history)
				print(f"âœ“ å·²åŠ è½½è®­ç»ƒå†å²ï¼Œå…± {self.step_count} æ­¥")
			else:
				self.clear_history()
		except Exception as e:
			print(f"âœ— åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
			self.clear_history()
	# ä¿å­˜è®­ç»ƒå†å²è®°å½•
	def save_history(self, verbose: bool = False):
	
		try:
			if not self.track_history:
				return
			np.savez(
				self.history_file,
				param_history=np.array(self.param_history),
				ripple_history=np.array(self.ripple_history),
				efficiency_history=np.array(self.efficiency_history),
				reward_history=np.array(self.reward_history),
				diversity_history=np.array(self.diversity_history),
				boundary_distance_history=np.array(self.boundary_distance_history)
			)
			# åªåœ¨ verbose=True æ—¶æ‰“å°
			if verbose:
				print(f"âœ“ å·²ä¿å­˜è®­ç»ƒå†å²ï¼ˆæ€»æ­¥æ•°: {self.step_count}ï¼‰")
		except Exception as e:
			print(f"âœ— ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

	#æ¸…ç©ºè®­ç»ƒå†å²trained_steps
	def clear_history(self):
		self.param_history = []
		self.ripple_history = []
		self.efficiency_history = []
		self.reward_history = []
		self.diversity_history = []
		self.boundary_distance_history = []
		self.step_count = 0
		if self.track_history:
			print("âœ“ å·²æ¸…ç©ºè®­ç»ƒå†å²")

	# ========== æ ¸å¿ƒå·¥å…·å‡½æ•° ==========
	"""
	å°†åŠ¨ä½œç©ºé—´[-1, 1]æ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´
		
	å‚æ•°:action: 5ç»´åŠ¨ä½œå‘é‡,èŒƒå›´[-1, 1]ï¼ˆä¸åŒ…å«é¢‘ç‡ï¼‰
			
	è¿”å›:6ç»´å‚æ•°å‘é‡,åŒ…å«å›ºå®šé¢‘ç‡å’Œ5ä¸ªå¯å˜å‚æ•° [f, L, C, Ron, RL, RC]
		"""
	def scale_action_to_params(self, action: np.ndarray) -> np.ndarray:
		# å°†5ç»´åŠ¨ä½œæ˜ å°„åˆ°å®é™…å‚æ•°èŒƒå›´
		params_without_freq = self.param_bounds[:, 0] + (action + 1) * 0.5 * self.param_ranges
		# åœ¨æœ€å‰é¢æ·»åŠ å›ºå®šé¢‘ç‡
		return np.concatenate([[self.fixed_frequency], params_without_freq])

	"""
	ä½¿ç”¨ä»£ç†æ¨¡å‹é¢„æµ‹Buckå˜æ¢å™¨æ€§èƒ½æŒ‡æ ‡
		
	å‚æ•°:params: 6ç»´å‚æ•°å‘é‡ [f, L, C, Ron, RL, RC]
			
	è¿”å›:Tuple[çº¹æ³¢ç³»æ•°, æ•ˆç‡]
	"""
	def predict_performance(self, params: np.ndarray) -> Tuple[float, float]:
		# æ ‡å‡†åŒ–è¾“å…¥å‚æ•°
		params_scaled = scaler_x.transform(params.reshape(1, -1))
		
		# ä½¿ç”¨ä»£ç†æ¨¡å‹é¢„æµ‹
		pred_scaled = surrogate_model.predict(params_scaled, verbose=0)
		
		# åæ ‡å‡†åŒ–è¾“å‡ºç»“æœ
		ripple, efficiency = scaler_y.inverse_transform(pred_scaled)[0]
		
		return ripple, efficiency

	# ç¯å¢ƒäº¤äº’æ ¸å¿ƒ 
	"""
	æ‰§è¡Œä¸€æ­¥ç¯å¢ƒäº¤äº’
		
	å‚æ•°:action: 5ç»´åŠ¨ä½œå‘é‡,èŒƒå›´[-1, 1]ï¼ˆä¸åŒ…å«é¢‘ç‡ï¼‰
			
	è¿”å›:Tuple[æ–°çŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»ˆæ­¢, æ˜¯å¦æˆªæ–­, ä¿¡æ¯å­—å…¸]
	"""
	def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
		self.current_step += 1
		
		# æ¢ç´¢ç­–ç•¥ï¼šéšæœºæ¢ç´¢å¢å¼ºå¤šæ ·æ€§
		if random.random() < Config.EXPLORATION_RATE:
			action = np.random.uniform(-1, 1, size=5)

		# åŠ¨ä½œæ˜ å°„åˆ°å®é™…å‚æ•°ï¼ˆåŒ…å«å›ºå®šé¢‘ç‡ï¼‰
		params = self.scale_action_to_params(action)
		
		# ä½¿ç”¨ä»£ç†æ¨¡å‹é¢„æµ‹æ€§èƒ½
		ripple, efficiency = self.predict_performance(params)

		# ç‰©ç†çº¦æŸæ£€æŸ¥å’Œè£å‰ª
		physical_violation = self._check_physical_constraints(efficiency, ripple)
		efficiency = np.clip(efficiency, self.min_efficiency, self.max_efficiency)
		ripple = np.clip(ripple, 0, 0.06)

		# è®¡ç®—å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
		reward = self._calculate_reward(params, ripple, efficiency, physical_violation)

		# æ›´æ–°çŠ¶æ€å’Œå†å²è®°å½•ï¼ˆçŠ¶æ€ä¸åŒ…å«å›ºå®šé¢‘ç‡ï¼‰
		self.state = np.concatenate([params[1:], [ripple, efficiency]])
		self._update_history(params, ripple, efficiency, reward)

		# å®šæœŸä¿å­˜å†å²ï¼ˆä»…è®­ç»ƒç¯å¢ƒï¼Œé™é»˜ä¿å­˜ï¼‰
		if self.track_history and self.step_count % Config.SAVE_FREQUENCY == 0:
			self.save_history(verbose=False)

		# Episodeç»ˆæ­¢æ¡ä»¶
		terminated = False
		truncated = self.current_step >= self.max_steps
		
		# æ„å»ºä¿¡æ¯å­—å…¸
		info = self._build_info_dict(params, ripple, efficiency, reward, physical_violation)
		
		return self.state, reward, terminated, truncated, info

	"""æ£€æŸ¥ç‰©ç†çº¦æŸæ˜¯å¦è¿å"""
	def _check_physical_constraints(self, efficiency: float, ripple: float) -> bool:
	
		return (efficiency < self.min_efficiency or efficiency > self.max_efficiency or
				ripple < 0 or ripple > 0.06)

	"""
	è®¡ç®—å¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
		
	å¥–åŠ±ç»„æˆï¼š
	1. æ•ˆç‡å¥–åŠ±ï¼šé¼“åŠ±é«˜æ•ˆç‡è®¾è®¡ï¼ˆæƒé‡åŠ å¤§ï¼‰
	2. çº¹æ³¢æƒ©ç½šï¼šæƒ©ç½šè¶…å‡ºçº¦æŸçš„çº¹æ³¢ï¼ˆæƒé‡åŠ å¤§ï¼‰
	3. è¾¹ç•Œå¥–åŠ±ï¼šé¼“åŠ±å‚æ•°è¿œç¦»è¾¹ç•Œ
	4. å¤šæ ·æ€§å¥–åŠ±ï¼šé¼“åŠ±å‚æ•°å¤šæ ·æ€§
	"""
	def _calculate_reward(self, params: np.ndarray, ripple: float, efficiency: float, 
						 physical_violation: bool) -> float:
		# 1. æ•ˆç‡å¥–åŠ±ï¼ˆä¸»è¦ç›®æ ‡ï¼Œæƒé‡åŠ å¤§ï¼‰
		eff_reward = 150 * (efficiency - 0.85)  # åŸºå‡†æ•ˆç‡85%ï¼Œæƒé‡ä»100æé«˜åˆ°150
		
		# æ•ˆç‡ç­‰çº§å¥–åŠ±ï¼ˆå¢å¼ºæ¿€åŠ±ï¼‰
		if efficiency >= 0.96:
			eff_reward += 30  # ä¼˜ç§€æ•ˆç‡ï¼ˆä»20æé«˜åˆ°30ï¼‰
		elif efficiency >= 0.93:
			eff_reward += 15  # è‰¯å¥½æ•ˆç‡ï¼ˆä»10æé«˜åˆ°15ï¼‰
		elif efficiency >= 0.90:
			eff_reward += 8   # å¯æ¥å—æ•ˆç‡ï¼ˆä»5æé«˜åˆ°8ï¼‰

		# 2. çº¹æ³¢æƒ©ç½šï¼ˆæƒé‡åŠ å¤§ï¼Œæ›´ä¸¥æ ¼ï¼‰
		ripple_penalty = 0.0
		if ripple > self.ripple_threshold:
			ripple_excess = (ripple - self.ripple_threshold) / self.ripple_threshold
			ripple_penalty = -3.0 * np.log(1 + ripple_excess)  # æƒé‡ä»-1.5æé«˜åˆ°-3.0

		# 3. è¾¹ç•Œè·ç¦»å¥–åŠ±ï¼ˆé¿å…å‚æ•°åœ¨è¾¹ç•Œé™„è¿‘ï¼Œä»…è€ƒè™‘å¯å˜å‚æ•°ï¼‰
		# params[0]æ˜¯å›ºå®šé¢‘ç‡ï¼Œä»ç´¢å¼•1å¼€å§‹æ˜¯å¯å˜å‚æ•°
		min_dist = min(
			min((params[i+1] - self.param_bounds[i, 0]) / self.param_ranges[i],
				(self.param_bounds[i, 1] - params[i+1]) / self.param_ranges[i])
			for i in range(len(self.param_bounds))
		)
		boundary_reward = 1.0 * min_dist if min_dist > 0.2 else 0.0

		# 4. å¤šæ ·æ€§å¥–åŠ±ï¼ˆé¼“åŠ±å‚æ•°æ¢ç´¢ï¼‰
		diversity_bonus = self._calculate_diversity_bonus(params)

		# ç‰©ç†çº¦æŸè¿åæƒ©ç½šï¼ˆåŠ å¤§æƒ©ç½šï¼‰
		if physical_violation:
			return -15.0  # ä»-10.0æé«˜åˆ°-15.0
		else:
			return eff_reward + ripple_penalty + boundary_reward + diversity_bonus

	"""è®¡ç®—å¤šæ ·æ€§å¥–åŠ±"""
	def _calculate_diversity_bonus(self, params: np.ndarray) -> float:
		# è¯„ä¼°ç¯å¢ƒæˆ–å†å²è¿‡çŸ­æ—¶ï¼Œä¸è®¡ç®—å¤šæ ·æ€§å¥–åŠ±
		if (not self.track_history) or (not self.param_history) or (len(self.param_history) < 5):
			return 0.0
			
		# ä¸æœ€è¿‘5ä¸ªå‚æ•°çš„å¹³å‡å·®å¼‚ï¼ˆä»…æ¯”è¾ƒå¯å˜å‚æ•°ï¼Œè·³è¿‡å›ºå®šé¢‘ç‡ï¼‰
		recent_history = np.array(self.param_history[-5:])
		avg_params = np.mean(recent_history, axis=0)
		# åªæ¯”è¾ƒå¯å˜å‚æ•°ï¼ˆç´¢å¼•1-5ï¼‰ï¼Œè·³è¿‡å›ºå®šé¢‘ç‡ï¼ˆç´¢å¼•0ï¼‰
		param_diff = np.abs(params[1:] - avg_params[1:]) / self.param_ranges
		diversity_bonus = min(np.mean(param_diff) * 2.0, 2.0)
		
		return diversity_bonus

	"""æ›´æ–°è®­ç»ƒå†å²è®°å½•"""
	def _update_history(self, params: np.ndarray, ripple: float, 
					   efficiency: float, reward: float):
		if self.track_history:
			self.param_history.append(params)
			self.ripple_history.append(ripple)
			self.efficiency_history.append(efficiency)
			self.reward_history.append(reward)
		
		# è®¡ç®—è¾¹ç•Œè·ç¦»ï¼ˆparams[0]æ˜¯å›ºå®šé¢‘ç‡ï¼Œä»ç´¢å¼•1å¼€å§‹æ˜¯å¯å˜å‚æ•°ï¼‰
		min_dist = min(
			min((params[i+1] - self.param_bounds[i, 0]) / self.param_ranges[i],
				(self.param_bounds[i, 1] - params[i+1]) / self.param_ranges[i])
			for i in range(len(self.param_bounds))
		)
		if self.track_history:
			self.boundary_distance_history.append(min_dist)
		
		# è®¡ç®—å¤šæ ·æ€§å¥–åŠ±
		diversity_bonus = self._calculate_diversity_bonus(params)
		if self.track_history:
			self.diversity_history.append(diversity_bonus)
		
		self.step_count += 1
		
	"""æ„å»ºä¿¡æ¯å­—å…¸"""
	def _build_info_dict(self, params: np.ndarray, ripple: float, efficiency: float,
						reward: float, physical_violation: bool) -> Dict:
		return {
			'params': params,
			'ripple': ripple,
			'efficiency': efficiency,
			'reward': reward,
			'ripple_violation': ripple > self.ripple_threshold,
			'physical_violation': physical_violation,
			'step': self.current_step,
			'total_steps': self.step_count
		}

	"""
	é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
		
	å‚æ•°:seed: éšæœºç§å­
		options: é‡ç½®é€‰é¡¹
			
	è¿”å›:Tuple[åˆå§‹çŠ¶æ€, ä¿¡æ¯å­—å…¸]
	"""
	def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:

		# è®¾ç½®éšæœºç§å­
		if seed is not None:
			np.random.seed(seed)
			random.seed(seed)

		# é‡ç½®episodeè®¡æ•°å™¨
		self.current_step = 0

		# åœ¨åˆç†èŒƒå›´å†…éšæœºåˆå§‹åŒ–å‚æ•°ï¼ˆä¸åŒ…å«é¢‘ç‡ï¼‰
		# é€‰æ‹©å‚æ•°èŒƒå›´çš„ä¸­å¿ƒåŒºåŸŸï¼Œé¿å…è¾¹ç•Œæ•ˆåº”
		random_params = [
			self.fixed_frequency,              # å¼€å…³é¢‘ç‡ï¼šå›ºå®š500kHz
			np.random.uniform(1.2e-6, 1.8e-6), # ç”µæ„Ÿï¼šä¸­å¿ƒåŒºåŸŸ
			np.random.uniform(8.5e-6, 9.5e-6), # ç”µå®¹ï¼šä¸­å¿ƒåŒºåŸŸ
			np.random.uniform(0.02, 0.06),     # å¼€å…³ç®¡ç”µé˜»ï¼šä¸­å¿ƒåŒºåŸŸ
			np.random.uniform(0.02, 0.06),     # ç”µæ„Ÿç”µé˜»ï¼šä¸­å¿ƒåŒºåŸŸ
			np.random.uniform(0.03, 0.08)      # ç”µå®¹ç”µé˜»ï¼šä¸­å¿ƒåŒºåŸŸ
		]
		
		# é¢„æµ‹åˆå§‹æ€§èƒ½
		ripple, efficiency = self.predict_performance(np.array(random_params))
		
		# æ„å»ºåˆå§‹çŠ¶æ€ï¼ˆä¸åŒ…å«å›ºå®šé¢‘ç‡ï¼ŒåªåŒ…å«5ä¸ªå¯å˜å‚æ•°å’Œ2ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼‰
		self.state = np.concatenate([random_params[1:], [ripple, efficiency]])
		
		return self.state, {}

	"""
	æ¸²æŸ“å½“å‰ç¯å¢ƒçŠ¶æ€
		
	å‚æ•°:mode: æ¸²æŸ“æ¨¡å¼
			
	è¿”å›:æ¸²æŸ“ç»“æœ(æ–‡æœ¬æ¨¡å¼è¿”å›å­—ç¬¦ä¸²,å…¶ä»–æ¨¡å¼è¿”å›None)
	"""
	def render(self, mode: str = 'human') -> Optional[str]:

		if mode == 'human':
			params_without_freq = self.state[:5]
			ripple, efficiency = self.state[5:]
			
			print("\n" + "="*50)
			print("           Buckå˜æ¢å™¨è®¾è®¡çŠ¶æ€")
			print("="*50)
			print(f"Episodeæ­¥æ•°: {self.current_step}/{self.max_steps}")
			print(f"æ€»è®­ç»ƒæ­¥æ•°: {self.step_count}")
			print("\nè®¾è®¡å‚æ•°:")
			print(f"  {'f(Hz)':>8}: {self.fixed_frequency:>12.6g}")
			for name, value in zip(self.param_names, params_without_freq):
				print(f"  {name:>8}: {value:>12.6g}")
			
			print(f"\næ€§èƒ½æŒ‡æ ‡:")
			print(f"  çº¹æ³¢ç³»æ•°: {ripple:>8.4f} ({'âœ“ æ»¡è¶³çº¦æŸ' if ripple <= self.ripple_threshold else 'âœ— è¶…å‡ºçº¦æŸ'})")
			print(f"  æ•ˆç‡:     {efficiency:>8.4f} ({efficiency * 100:>6.2f}%)")
			print("="*50)
			
		return None

"""
åˆ›å»ºå¹¶é…ç½®PPOæ¨¡å‹
	
å‚æ•°:env: Buckå˜æ¢å™¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
		
è¿”å›:é…ç½®å¥½çš„PPOæ¨¡å‹
"""
# PPOæ¨¡å‹é…ç½®ä¸è®­ç»ƒè®¾ç½®
def create_ppo_model(env) -> PPO:

	print("æ­£åœ¨åˆ›å»ºPPOæ¨¡å‹...")
	
	# åˆ›å»ºTensorBoardæ—¥å¿—ç›®å½•
	os.makedirs(Config.TENSORBOARD_LOG, exist_ok=True)
	
	# åˆ›å»ºPPOæ¨¡å‹
	model = PPO(
		"MlpPolicy",                    # ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥
		env,                            # ç¯å¢ƒï¼ˆå·²ç›‘æ§/å½’ä¸€åŒ–ï¼‰
		verbose=1,                      # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
		tensorboard_log=Config.TENSORBOARD_LOG,  # TensorBoardæ—¥å¿—
		**Config.PPO_CONFIG             # ä½¿ç”¨é…ç½®ä¸­çš„è¶…å‚æ•°
	)
	
	print("âœ“ PPOæ¨¡å‹åˆ›å»ºå®Œæˆ")
	print(f"  å­¦ä¹ ç‡: {Config.PPO_CONFIG['learning_rate']}")
	print(f"  æ‰¹å¤§å°: {Config.PPO_CONFIG['batch_size']}")
	print(f"  æ›´æ–°æ­¥æ•°: {Config.PPO_CONFIG['n_steps']}")
	print(f"  è®­ç»ƒè½®æ•°: {Config.PPO_CONFIG['n_epochs']}")
	
	return model

"""
åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°
	
è¿”å›:å›è°ƒå‡½æ•°åˆ—è¡¨
"""
def create_training_callbacks() -> List[BaseCallback]:

	callbacks = []
	
	# ç¡®ä¿ä¿å­˜ä¸æ—¥å¿—ç›®å½•å­˜åœ¨
	os.makedirs(Config.MODEL_SAVE_PATH, exist_ok=True)
	os.makedirs(Config.CHECKPOINT_PATH, exist_ok=True)
	os.makedirs(os.path.join(Config.TENSORBOARD_LOG, 'Eval/'), exist_ok=True)

	# 1. è¯„ä¼°å›è°ƒ - å®šæœŸè¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
	eval_callback = EvalCallback(
		VecMonitor(load_or_create_vec_env(training=False, track_history=False)),
		best_model_save_path=Config.MODEL_SAVE_PATH,
		log_path=os.path.join(Config.TENSORBOARD_LOG, 'Eval/'),
		eval_freq=Config.EVAL_FREQUENCY,
		deterministic=False,
		verbose=1
	)
	callbacks.append(eval_callback)
	
	# 2. æ£€æŸ¥ç‚¹å›è°ƒ - å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
	checkpoint_callback = CheckpointCallback(
		save_freq=Config.CHECKPOINT_FREQUENCY,
		save_path=Config.CHECKPOINT_PATH,
		name_prefix="buck_ppo"
	)
	callbacks.append(checkpoint_callback)
	
	print("âœ“ è®­ç»ƒå›è°ƒå‡½æ•°åˆ›å»ºå®Œæˆ")
	return callbacks

def make_base_env(track_history: bool = True) -> DummyVecEnv:
	return DummyVecEnv([lambda: BuckConverterEnv(track_history=track_history)])

def load_or_create_vec_env(training: bool = True, track_history: bool = True) -> VecNormalize:
	"""åˆ›å»ºæˆ–åŠ è½½å¸¦å½’ä¸€åŒ–çš„ç¯å¢ƒ"""
	base_env = make_base_env(track_history=track_history)
	if os.path.exists(Config.VECNORM_PATH):
		try:
			vec_env = VecNormalize.load(Config.VECNORM_PATH, base_env)
			vec_env.training = training
			vec_env.norm_reward = True
			vec_env.norm_obs = True
			print(f"âœ“ å·²åŠ è½½VecNormalizeç»Ÿè®¡: {Config.VECNORM_PATH} (training={training})")
			return vec_env
		except Exception as e:
			# æ£€æŸ¥æ˜¯å¦æ˜¯è§‚æµ‹ç©ºé—´ä¸åŒ¹é…
			if "observation_space" in str(e).lower() or "shape" in str(e).lower():
				print(f"âš ï¸ VecNormalizeä¸å½“å‰ç¯å¢ƒä¸å…¼å®¹ï¼ˆè§‚æµ‹ç©ºé—´å·²æ›´æ”¹ï¼‰")
				print(f"âš ï¸ å°†å¤‡ä»½æ—§æ–‡ä»¶å¹¶åˆ›å»ºæ–°çš„VecNormalize")
				# å¤‡ä»½æ—§æ–‡ä»¶
				import shutil
				import time
				timestamp = time.strftime("%Y%m%d_%H%M%S")
				backup_path = Config.VECNORM_PATH.replace('.pkl', f'_backup_{timestamp}.pkl')
				try:
					shutil.move(Config.VECNORM_PATH, backup_path)
					print(f"âœ“ æ—§VecNormalizeå·²å¤‡ä»½åˆ°: {backup_path}")
				except Exception as backup_error:
					print(f"âš ï¸ å¤‡ä»½å¤±è´¥: {backup_error}")
			else:
				print(f"âœ— åŠ è½½VecNormalizeå¤±è´¥ï¼Œä½¿ç”¨æ–°ç»Ÿè®¡: {e}")
	# æ–°å»º
	vec_env = VecNormalize(
		base_env,
		norm_obs=True,
		norm_reward=True,
		clip_obs=10.0,
		clip_reward=10.0
	)
	vec_env.training = training
	print("âœ“ å·²åˆ›å»ºæ–°çš„VecNormalizeç¯å¢ƒ")
	return vec_env

def get_inner_env(vec_env) -> BuckConverterEnv:
	"""è·å–æœ€å†…å±‚çš„ BuckConverterEnv å®ä¾‹ (ç¬¬0ä¸ªå­ç¯å¢ƒ)"""
	e = vec_env
	# è§£åŒ… VecEnvWrapper é“¾
	while hasattr(e, 'venv'):
		e = e.venv
	# å–ç¬¬ä¸€ä¸ªå­ç¯å¢ƒ
	if hasattr(e, 'envs') and len(e.envs) > 0:
		return e.envs[0]
	return e

# æ£€æŸ¥ç‚¹å›è°ƒç±»å®šä¹‰
"""
å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹çš„å›è°ƒå‡½æ•°
"""
class CheckpointCallback(BaseCallback):

	"""
	åˆå§‹åŒ–æ£€æŸ¥ç‚¹å›è°ƒ
		
	å‚æ•°:save_freq: ä¿å­˜é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
		save_path: ä¿å­˜è·¯å¾„
		name_prefix: æ–‡ä»¶åå‰ç¼€
	"""
	def __init__(self, save_freq: int, save_path: str, name_prefix: str = "ppo_model"):

		super(CheckpointCallback, self).__init__()
		self.save_freq = save_freq
		self.save_path = save_path
		self.name_prefix = name_prefix
	
	"""åˆå§‹åŒ–å›è°ƒï¼Œåˆ›å»ºä¿å­˜ç›®å½•"""
	def _init_callback(self) -> None:
		if self.save_path is not None:
			os.makedirs(self.save_path, exist_ok=True)
	
	"""æ¯æ­¥æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æ£€æŸ¥ç‚¹"""
	def _on_step(self) -> bool:
		if self.n_calls % self.save_freq == 0:
			path = os.path.join(self.save_path, f"{self.name_prefix}_{self.num_timesteps}_steps")
			self.model.save(path)
			print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: æ­¥æ•° {self.num_timesteps} -> {path}")
		return True

# åˆ›å»ºç¯å¢ƒï¼ˆå¸¦VecNormalizeä¸ç›‘æ§ï¼‰ä¸å›è°ƒ
print("æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...")
try:
	env = load_or_create_vec_env(training=True, track_history=True)
	env = VecMonitor(env)
	print("âœ“ Buckå˜æ¢å™¨ç¯å¢ƒåˆ›å»ºå®Œæˆ (VecNormalize)")
except Exception as e:
	print(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
	import traceback
	traceback.print_exc()
	raise

# åˆ›å»ºè®­ç»ƒå›è°ƒ
callbacks = create_training_callbacks()

# è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
"""
ç»˜åˆ¶è®­ç»ƒè¿›åº¦å…³é”®æŒ‡æ ‡
	
å‚æ•°:env: Buckå˜æ¢å™¨ç¯å¢ƒ(åŒ…å«å†å²æ•°æ®)
	save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
"""
def plot_training_progress(env: BuckConverterEnv, save_dir: str = "E:/AI-based optimized design/Visualization/") -> None:

	os.makedirs(save_dir, exist_ok=True)
	
	if not env.reward_history:
		print("âš ï¸ æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
		return
	
	print("æ­£åœ¨ç”Ÿæˆè®­ç»ƒè¿›åº¦å¯è§†åŒ–...")
	
	# åˆ›å»ºå›¾å½¢
	fig, axes = plt.subplots(2, 2, figsize=(12, 10))
	fig.suptitle('Buckå˜æ¢å™¨PPOè®­ç»ƒè¿›åº¦', fontsize=16, fontweight='bold')
	
	# 1. å¥–åŠ±æ›²çº¿
	axes[0, 0].plot(env.reward_history, alpha=0.7, linewidth=1)
	axes[0, 0].set_title('è®­ç»ƒå¥–åŠ±å˜åŒ–')
	axes[0, 0].set_xlabel('è®­ç»ƒæ­¥æ•°')
	axes[0, 0].set_ylabel('å¥–åŠ±å€¼')
	axes[0, 0].grid(True, alpha=0.3)
	
	# 2. æ•ˆç‡å˜åŒ–
	axes[0, 1].plot(env.efficiency_history, alpha=0.7, linewidth=1, color='green')
	axes[0, 1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='ç›®æ ‡æ•ˆç‡ 95%')
	axes[0, 1].set_title('æ•ˆç‡å˜åŒ–')
	axes[0, 1].set_xlabel('è®­ç»ƒæ­¥æ•°')
	axes[0, 1].set_ylabel('æ•ˆç‡')
	axes[0, 1].legend()
	axes[0, 1].grid(True, alpha=0.3)
	
	# 3. çº¹æ³¢å˜åŒ–
	axes[1, 0].plot(env.ripple_history, alpha=0.7, linewidth=1, color='orange')
	axes[1, 0].axhline(y=Config.RIPPLE_THRESHOLD, color='red', linestyle='--', alpha=0.7, label='çº¹æ³¢é˜ˆå€¼ 2%')
	axes[1, 0].set_title('çº¹æ³¢ç³»æ•°å˜åŒ–')
	axes[1, 0].set_xlabel('è®­ç»ƒæ­¥æ•°')
	axes[1, 0].set_ylabel('çº¹æ³¢ç³»æ•°')
	axes[1, 0].legend()
	axes[1, 0].grid(True, alpha=0.3)
	
	# 4. æ•ˆç‡vsçº¹æ³¢æ•£ç‚¹å›¾
	scatter = axes[1, 1].scatter(env.efficiency_history, env.ripple_history, 
								 c=env.reward_history, cmap='viridis', alpha=0.6)
	axes[1, 1].axhline(y=Config.RIPPLE_THRESHOLD, color='red', linestyle='--', alpha=0.7, label='çº¹æ³¢é˜ˆå€¼')
	axes[1, 1].axvline(x=0.95, color='green', linestyle='--', alpha=0.7, label='ç›®æ ‡æ•ˆç‡')
	axes[1, 1].set_title('æ•ˆç‡ vs çº¹æ³¢ (é¢œè‰²=å¥–åŠ±)')
	axes[1, 1].set_xlabel('æ•ˆç‡')
	axes[1, 1].set_ylabel('çº¹æ³¢ç³»æ•°')
	axes[1, 1].legend()
	axes[1, 1].grid(True, alpha=0.3)
	plt.colorbar(scatter, ax=axes[1, 1], label='å¥–åŠ±å€¼')
	
	plt.tight_layout()
	
	# ä¿å­˜å›¾ç‰‡
	save_path = os.path.join(save_dir, 'training_progress.png')
	plt.savefig(save_path, dpi=300, bbox_inches='tight')
	plt.close()
	
	print(f"âœ“ è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜: {save_path}")

# def plot_parameter_evolution(env: BuckConverterEnv, save_dir: str = "E:/AI-based optimized design/Visualization/") -> None:
# 	"""
# 	ç»˜åˆ¶å‚æ•°æ¼”åŒ–è¿‡ç¨‹
	
# 	Args:
# 		env: Buckå˜æ¢å™¨ç¯å¢ƒ
# 		save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
# 	"""
# 	if not env.param_history:
# 		print("âš ï¸ æ²¡æœ‰å‚æ•°å†å²æ•°æ®ï¼Œè·³è¿‡å‚æ•°æ¼”åŒ–å›¾")
# 		return
	
# 	print("æ­£åœ¨ç”Ÿæˆå‚æ•°æ¼”åŒ–å›¾...")
	
# 	param_history = np.array(env.param_history)
# 	param_names = list(Config.PARAM_BOUNDS.keys())
	
# 	fig, axes = plt.subplots(2, 3, figsize=(15, 10))
# 	fig.suptitle('Buckå˜æ¢å™¨å‚æ•°æ¼”åŒ–è¿‡ç¨‹', fontsize=16, fontweight='bold')
# 	axes = axes.flatten()
	
# 	for i, (name, bounds) in enumerate(Config.PARAM_BOUNDS.items()):
# 		ax = axes[i]
# 		ax.plot(param_history[:, i], alpha=0.7, linewidth=1)
# 		ax.axhline(y=bounds[0], color='red', linestyle='--', alpha=0.5, label='ä¸‹ç•Œ')
# 		ax.axhline(y=bounds[1], color='red', linestyle='--', alpha=0.5, label='ä¸Šç•Œ')
# 		ax.set_title(f'{name} æ¼”åŒ–')
# 		ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
# 		ax.set_ylabel('å‚æ•°å€¼')
# 		ax.legend()
# 		ax.grid(True, alpha=0.3)
	
# 	plt.tight_layout()
	
# 	# ä¿å­˜å›¾ç‰‡
# 	save_path = os.path.join(save_dir, 'parameter_evolution.png')
# 	plt.savefig(save_path, dpi=300, bbox_inches='tight')
# 	plt.close()
	
# 	print(f"âœ“ å‚æ•°æ¼”åŒ–å›¾å·²ä¿å­˜: {save_path}")

# è®­ç»ƒä¸»æµç¨‹
"""
PPOæ¨¡å‹è®­ç»ƒä¸»å‡½æ•°
	
å‚æ•°:total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
	batch_size: æ¯æ‰¹è®­ç»ƒæ­¥æ•°
		
è¿”å›:è®­ç»ƒå¥–åŠ±å†å²
"""
def train_ppo_model(total_timesteps: int = 36000, batch_size: int = 4096) -> List[float]:

	print("å¼€å§‹PPOè®­ç»ƒ")
	print("="*60)
	
	# æ¨¡å‹è·¯å¾„é…ç½®
	ppo_model_path = os.path.join(Config.MODEL_SAVE_PATH, 'buck_optimizer_ppo')
	checkpoint_path = ppo_model_path + '_checkpoint'
	
	# æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹
	if os.path.exists(checkpoint_path + '.zip'):
		print(f"âœ“ å‘ç°æ£€æŸ¥ç‚¹: {checkpoint_path}")
		try:
			# å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
			model = PPO.load(checkpoint_path, env=env)
			model.verbose = 1  # è®¾ç½®ä¸ºæ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
			trained_steps = model.num_timesteps
			print(f"âœ“ å·²æ¢å¤è®­ç»ƒï¼Œå½“å‰æ­¥æ•°: {trained_steps}")
		except (ValueError, AssertionError) as e:
			# è§‚æµ‹ç©ºé—´æˆ–åŠ¨ä½œç©ºé—´ä¸åŒ¹é…ï¼ˆç‰ˆæœ¬å‡çº§å¯¼è‡´ï¼‰
			if "spaces do not match" in str(e) or "observation_space" in str(e):
				print(f"âš ï¸ æ£€æŸ¥ç‚¹ä¸å½“å‰ç¯å¢ƒä¸å…¼å®¹ï¼ˆè§‚æµ‹ç©ºé—´å·²æ›´æ”¹ï¼‰")
				print(f"âš ï¸ å°†å¤‡ä»½æ—§æ£€æŸ¥ç‚¹å¹¶å¼€å§‹å…¨æ–°è®­ç»ƒ")
				
				# å¤‡ä»½æ—§æ–‡ä»¶
				import shutil
				import time
				timestamp = time.strftime("%Y%m%d_%H%M%S")
				backup_dir = os.path.join(Config.CHECKPOINT_PATH, f'backup_{timestamp}')
				os.makedirs(backup_dir, exist_ok=True)
				
				if os.path.exists(checkpoint_path + '.zip'):
					shutil.move(checkpoint_path + '.zip', 
							   os.path.join(backup_dir, 'buck_optimizer_ppo_checkpoint.zip'))
					print(f"âœ“ æ—§æ£€æŸ¥ç‚¹å·²å¤‡ä»½åˆ°: {backup_dir}")
				
				if os.path.exists(Config.VECNORM_PATH):
					shutil.move(Config.VECNORM_PATH, 
							   os.path.join(backup_dir, 'vecnormalize.pkl'))
					print(f"âœ“ æ—§VecNormalizeå·²å¤‡ä»½")
				
				# å¼€å§‹å…¨æ–°è®­ç»ƒ
				model = create_ppo_model(env)
				trained_steps = 0
				# æ¸…ç©ºåº•å±‚ç¯å¢ƒå†å²
				try:
					inner_env = get_inner_env(env)
					inner_env.clear_history()
					print("âœ“ å·²æ¸…ç©ºè®­ç»ƒå†å²ï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ")
				except Exception as e:
					print(f"âš ï¸ æ— æ³•æ¸…ç©ºå†å²(éè‡´å‘½): {e}")
			else:
				# å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
				raise
	else:
		print("âœ“ å¼€å§‹å…¨æ–°è®­ç»ƒä¼šè¯")
		model = create_ppo_model(env)
		trained_steps = 0
		# æ¸…ç©ºåº•å±‚ç¯å¢ƒå†å²
		try:
			inner_env = get_inner_env(env)
			inner_env.clear_history()
		except Exception as e:
			print(f"âš ï¸ æ— æ³•æ¸…ç©ºå†å²(éè‡´å‘½): {e}")

	# è®¡ç®—è®­ç»ƒæ‰¹æ¬¡
	remaining_steps = total_timesteps - trained_steps
	if remaining_steps < 0:
		remaining_steps = 0;
	batches = math.ceil(remaining_steps / batch_size)
	
	print(f"è®­ç»ƒé…ç½®:")
	print(f"  æ€»æ­¥æ•°: {total_timesteps}")
	print(f"  æ‰¹å¤§å°: {batch_size}")
	print(f"  å‰©ä½™æ­¥æ•°: {remaining_steps}")
	print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {batches}")
	print("="*60)

	# åˆ†æ‰¹è®­ç»ƒ
	for batch in range(batches):
		print(f"\n{'='*60}")
		print(f"ğŸ”„ è®­ç»ƒæ‰¹æ¬¡ {batch+1}/{batches}")
		print(f"   ç›®æ ‡æ­¥æ•°: {trained_steps} -> {min(trained_steps + batch_size, total_timesteps)}")
		print(f"{'='*60}")

		current_batch_size = min(batch_size, total_timesteps - trained_steps)

		# æ‰§è¡Œè®­ç»ƒ
		print(f"\nå¼€å§‹è®­ç»ƒ {current_batch_size} æ­¥...")
		model.learn(
			total_timesteps=current_batch_size,
			callback=callbacks,
			reset_num_timesteps=False,
			tb_log_name="PPO_Buck1",
			progress_bar=True
		)

		trained_steps = model.num_timesteps

		# æ˜¾ç¤ºè®­ç»ƒè¿›åº¦ç»Ÿè®¡
		try:
			inner_env = get_inner_env(env)
			if inner_env.reward_history:
				recent_rewards = inner_env.reward_history[-100:]
				recent_efficiency = inner_env.efficiency_history[-100:]
				recent_ripple = inner_env.ripple_history[-100:]
				print(f"\nğŸ“Š æœ€è¿‘100æ­¥ç»Ÿè®¡:")
				print(f"   å¹³å‡å¥–åŠ±: {np.mean(recent_rewards):>8.2f}")
				print(f"   å¹³å‡æ•ˆç‡: {np.mean(recent_efficiency):>8.4f} ({np.mean(recent_efficiency)*100:.2f}%)")
				print(f"   å¹³å‡çº¹æ³¢: {np.mean(recent_ripple):>8.4f}")
				print(f"   æ€»è®­ç»ƒæ­¥æ•°: {inner_env.step_count}")
		except Exception as e:
			pass

		# ä¿å­˜æ£€æŸ¥ç‚¹ã€VecNormalizeç»Ÿè®¡å’Œå†å²
		model.save(checkpoint_path)
		try:
			os.makedirs(os.path.dirname(Config.VECNORM_PATH), exist_ok=True)
			env.save(Config.VECNORM_PATH)
		except Exception as e:
			print(f"âœ— ä¿å­˜VecNormalizeå¤±è´¥: {e}")
		# ä¿å­˜åº•å±‚ç¯å¢ƒå†å²ï¼ˆverbose=True æ˜¾ç¤ºä¿å­˜ä¿¡æ¯ï¼‰
		try:
			inner_env = get_inner_env(env)
			inner_env.save_history(verbose=True)
		except Exception as e:
			print(f"âš ï¸ æ— æ³•ä¿å­˜å†å²(éè‡´å‘½): {e}")
		print(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜ -> {checkpoint_path}")

	# è®­ç»ƒå®Œæˆ
	print("è®­ç»ƒå®Œæˆ")
	print("="*60)
	
	# ä¿å­˜æœ€ç»ˆæ¨¡å‹ä¸VecNormalizeç»Ÿè®¡
	print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
	model.save(ppo_model_path)
	try:
		env.save(Config.VECNORM_PATH)
	except Exception as e:
		print(f"âœ— ä¿å­˜æœ€ç»ˆVecNormalizeå¤±è´¥: {e}")
	# ä¿å­˜åº•å±‚ç¯å¢ƒå†å²
	try:
		inner_env = get_inner_env(env)
		inner_env.save_history(verbose=True)
	except Exception as e:
		print(f"âš ï¸ æœ€ç»ˆä¿å­˜å†å²å¤±è´¥(éè‡´å‘½): {e}")
	print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {ppo_model_path}")
	
	# ç”Ÿæˆå¯è§†åŒ–
	print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–...")
	# ä½¿ç”¨åº•å±‚ç¯å¢ƒæ•°æ®è¿›è¡Œå¯è§†åŒ–
	try:
		inner_env = get_inner_env(env)
		plot_training_progress(inner_env)
	except Exception as e:
		print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥(éè‡´å‘½): {e}")
	
	# è¿”å›åº•å±‚ç¯å¢ƒçš„å¥–åŠ±å†å²
	try:
		inner_env = get_inner_env(env)
		return inner_env.reward_history
	except Exception:
		return []

# ä¸»ç¨‹åºå…¥å£
def main():
	print("ğŸš€ Buckå˜æ¢å™¨PPOä¼˜åŒ–ç³»ç»Ÿå¯åŠ¨")
	print("="*60)
	
	# æ˜¾ç¤ºé…ç½®ä¿¡æ¯
	print("ğŸ“‹ å½“å‰é…ç½®:")
	print(f"  ä»£ç†æ¨¡å‹: {Config.MODEL_PATH}")
	print(f"  å‚æ•°èŒƒå›´: {len(Config.PARAM_BOUNDS)} ä¸ªå‚æ•°")
	print(f"  çº¹æ³¢é˜ˆå€¼: {Config.RIPPLE_THRESHOLD*100:.1f}%")
	print(f"  æ•ˆç‡èŒƒå›´: {Config.MIN_EFFICIENCY*100:.1f}% - {Config.MAX_EFFICIENCY*100:.1f}%")
	print(f"  PPOå­¦ä¹ ç‡: {Config.PPO_CONFIG['learning_rate']}")
	print(f"  æ‰¹å¤§å°: {Config.PPO_CONFIG['batch_size']}")
	print("="*60)
	
	# å¼€å§‹è®­ç»ƒ
	try:
		reward_history = train_ppo_model(
			total_timesteps=36000,
			batch_size=4096
		)
		
		# æ˜¾ç¤ºè®­ç»ƒç»“æœæ‘˜è¦
		if reward_history:
			final_reward = np.mean(reward_history[-100:])  # æœ€å100æ­¥å¹³å‡å¥–åŠ±
			max_reward = np.max(reward_history)
			print(f"\nğŸ“ˆ è®­ç»ƒç»“æœæ‘˜è¦:")
			print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_reward:.2f}")
			print(f"  æœ€é«˜å¥–åŠ±: {max_reward:.2f}")
		
		print("\nâœ… è®­ç»ƒå®Œæˆï¼")
		print("ğŸ“ æ£€æŸ¥ä»¥ä¸‹ç›®å½•è·å–ç»“æœ:")
		print(f"  - æ¨¡å‹æ–‡ä»¶: {Config.MODEL_SAVE_PATH}")
		print(f"  - å¯è§†åŒ–: E:/AI-based optimized design/Visualization/")
		print(f"  - TensorBoard: {Config.TENSORBOARD_LOG}")
		
	except Exception as e:
		print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
		import traceback
		traceback.print_exc()

if __name__ == '__main__':
	main()